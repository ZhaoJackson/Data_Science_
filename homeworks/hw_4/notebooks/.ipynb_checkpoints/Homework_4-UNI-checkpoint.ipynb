{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "### Due: Mon Dec 16 @ 11:59pm ET\n",
    "\n",
    "# NLP: Recommendations and Sentiment Analysis\n",
    "\n",
    "In this homework we will perform two common NLP tasks: \n",
    " 1. Generate recommendations for products based on product descriptions using an LDA topic model.\n",
    " 2. Perform sentiment analysis based on product reviews using sklearn Pipelines.\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Replace Name and UNI in the first cell and filename\n",
    "- Follow the comments below and fill in the blanks (\\_\\_\\_\\_) to complete.\n",
    "- Where not specified, please run functions with default argument settings.\n",
    "- Please **'Restart and Run All'** prior to submission.\n",
    "- **Save pdf in Landscape** and **check that all of your code is shown** in the submission.\n",
    "- When submitting in Gradescope, be sure to **select which page corresponds to which question.**\n",
    "\n",
    "Out of 50 points total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. (2pts total) Homework Submission\n",
    "\n",
    "# (1pt) The homework should be spread over multiple pdf pages, not one single pdf page\n",
    "# (1pt) When submitting, assign each question to the pdf page where the solution is printed.\n",
    "#        If there is no print statement for a question, assign the question to the first pdf \n",
    "#        page where the code for the question is visible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Generate Recommendations from LDA Transformation\n",
    "\n",
    "In this part we will transform a set of product descriptions using TfIdf and LDA topic modeling to generate product recommendations based on similarity in LDA space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and transform text using TfIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   name_title   5000 non-null   object\n",
      " 1   description  5000 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 78.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 2. (1pts) Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product descriptions \n",
    "#   from the JCPenney department store.\n",
    "\n",
    "# Load product information from ../data/jcpenney-products_subset.csv.zip\n",
    "# Use pandas read_csv function with the default parameters.\n",
    "# Note that this is a compressed version of a csv file (has a .zip suffix).\n",
    "# .read_csv() has a parameter 'compression' with default \n",
    "#     value 'infer' that will handle unzipping the data for us.\n",
    "# Store the resulting dataframe as df_jcp.\n",
    "df_jcp = pd.read_csv('../data/jcpenney-products_subset.csv.zip', compression = 'infer')\n",
    "\n",
    "# print a summary of df_jcp using .info()\n",
    "# there should be 5000 rows with 2 columns with no missing data\n",
    "print(df_jcp.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product Name: Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012\n",
      "--------------------------------------------------\n",
      "Product Description: A timepiece you can enjoy every day of the week, this sports car-inspired chronograph watch packs plenty of information into an easy-to-read dial.   Brand: Invicta Dial Color: Black Strap: Black leather Clasp: Buckle Movement: Quartz Water Resistance: 100m Case Width: 48mm Case Thickness: 13.5mm Bracelet Dimensions: 210mm long; 22mm wide Model No.: 16012 Special Features: Stopwatch; 3 multifunction sub dials   Jewelry photos are enlarged to show detail.\n"
     ]
    }
   ],
   "source": [
    "# 3. (2pts) Print an Example\n",
    "\n",
    "# The two columns of the dataframe we're interested in are:\n",
    "#   'name_title' which is the name of the product stored as a string\n",
    "#   'description' which is a description of the product stored as a string\n",
    "#\n",
    "# We'll print out the product in the first row as an example\n",
    "# If we try to print both at the same time, pandas will truncate the strings\n",
    "#   so we'll print them seperately\n",
    "\n",
    "# print the name_title column in row 0 of df_jcp\n",
    "print('Product Name:', df_jcp.loc[0, 'name_title'])\n",
    "\n",
    "# printing a line of dashes\n",
    "print('-'*50) \n",
    "\n",
    "# print the desciption column in row 0 of df_jcp\n",
    "print(\"Product Description:\", df_jcp.loc[0, 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 5678)\n"
     ]
    }
   ],
   "source": [
    "# 4. (4pts) Transform Descriptions using TfIdf\n",
    "\n",
    "# In order to pass our product descriptions to the LDA model, we first\n",
    "#   need to vectorize from strings to fixed length vectors of floats.\n",
    "# To do this we will transform our documents into a TfIdf representation.\n",
    "\n",
    "# Import TfidfVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#  Instantiate a TfidfVectorizer that will\n",
    "#    use both unigrams + bigrams\n",
    "#    exclude terms which appear in less than 10 documents\n",
    "#    exclude terms which appear in more than 10% of the documents\n",
    "#    all other parameters leave as default\n",
    "# Store as tfidf\n",
    "tfidf = TfidfVectorizer(ngram_range = (1,2), min_df=10, max_df=.10)\n",
    "\n",
    "# fit_transform() tfidf on the description column of df_jcp, creating the transformed dataset X_tfidf\n",
    "# Store as X_tfidf\n",
    "X_tfidf = tfidf.fit_transform(df_jcp.description)\n",
    "\n",
    "# Print the shape of X_tfidf (should be 5000 x 5678)\n",
    "print(X_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['jewelry photos', 'features stopwatch', 'special features',\n",
       "        'model no', 'wide model', '22mm wide', 'long 22mm',\n",
       "        'bracelet dimensions', 'case thickness', 'case width',\n",
       "        'resistance 100m', 'water resistance', 'quartz water',\n",
       "        'movement quartz', 'buckle movement', 'clasp buckle',\n",
       "        'leather clasp', 'black leather', 'strap black', 'black strap',\n",
       "        'color black', 'dial color', 'to read', 'easy to', 'an easy',\n",
       "        'plenty of', 'of the', 'day of', 'every day', 'you can', 'sub',\n",
       "        'stopwatch', 'special', 'no', 'model', 'wide', '22mm',\n",
       "        'dimensions', 'bracelet', '5mm', '13', 'thickness', 'width',\n",
       "        'case', '100m', 'resistance', 'water', 'quartz', 'movement',\n",
       "        'buckle', 'clasp', 'leather', 'strap', 'black', 'color', 'brand',\n",
       "        'dial', 'read', 'into', 'plenty', 'watch', 'chronograph',\n",
       "        'inspired', 'car', 'sports', 'week', 'day', 'every', 'enjoy',\n",
       "        'can'], dtype='<U24')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5: (1pts) Show The Terms Extracted From Row 0\n",
    "\n",
    "# X_tfidf is a matrix of floats, one row per document, one column per vocab term\n",
    "# We can see what terms were extracted, and kept, for the document at df_jcp row 0\n",
    "#   using the .inverse_transform() function\n",
    "# Print the result of calling:\n",
    "#   the .inverse_transform() function of tfidf on the first row of X_tfidf\n",
    "# You should see an array starting with 'jewelry photos'\n",
    "tfidf.inverse_transform(X_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zipper_pocket', 'zipper_pockets', 'zippered', 'zirconia', 'zone']\n"
     ]
    }
   ],
   "source": [
    "# 6. (3pts) Format Bigrams and Print Sample of Extracted Vocabulary \n",
    "\n",
    "# The learned vocabulary can be retrieved from tfidf as a list using .get_feature_names_out()\n",
    "# Store the extracted vocabulary as vocab\n",
    "vocab = tfidf.get_feature_names_out()\n",
    "\n",
    "# Sklearn joins bigrams with a space character.\n",
    "# To make our output easier to read, replace the spaces in each term in \n",
    "#    vocab (a list of strings) with an underscore.\n",
    "# To do this we can use the string .replace() method.\n",
    "# For example x.replace(' ','_') will replace all ' ' in x with '_'.\n",
    "# Store the result back into vocab\n",
    "vocab = [x.replace(' ', '_') for x in vocab]\n",
    "\n",
    "# Print the last 5 terms in the vocab\n",
    "#  The first term printed should be 'zipper_pocket'\n",
    "print(vocab[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform product descriptions into topics and print sample terms from topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 20)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7. (3pts) Perform Topic Modeling with LDA\n",
    "\n",
    "# Now that we have our vectorized data, we can use Latent Direchlet Allocation to learn \n",
    "#   per-document topic distributions and per-topic term distributions.\n",
    "# Though the documents are likely composed of more, we'll model our dataset using \n",
    "#     20 topics for ease of printing.\n",
    "\n",
    "# Import LatentDirichletAllocation from sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Instantiate a LatentDirichletAllocation model that will\n",
    "#    produce 20 topics\n",
    "#    use all available cores to train\n",
    "#    random_state=512\n",
    "# Store as lda\n",
    "lda = LatentDirichletAllocation(n_components = 20, \n",
    "                                n_jobs = -1, \n",
    "                                random_state=512)\n",
    "\n",
    "# Run fit_transform on lda using X_tfidf.\n",
    "# Store the output (the per-document topic distributions) as X_lda\n",
    "X_lda = lda.fit_transform(X_tfidf)\n",
    "\n",
    "# Print the shape of the X_lda (should be 5000 x 20)\n",
    "X_lda.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "theta_0 = [0.01 0.78 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.05 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01 0.01 0.01 0.08]\n",
      "\n",
      "n_topics_assigned_0 = 3\n",
      "\n",
      "assigned_topics_0 = [1 2 0]\n"
     ]
    }
   ],
   "source": [
    "# 8. (5pts) Get Assigned Topics for Product at df_jcp row 0\n",
    "\n",
    "# Get the assigned topic proportions for the document at row 0 of X_lda\n",
    "# This will be a list of 20 floats between 0 and 1\n",
    "# Round all values to a precision of 2\n",
    "# Store as theta_0\n",
    "theta_0 = np.round(X_lda[0], 2)\n",
    "print(f'{theta_0 = :}\\n')\n",
    "\n",
    "# LDA will assign a small weight (or proability) to each topic for a document\n",
    "# How many of the topics in theta_0 have a (relatively) large weight (> .01)?\n",
    "# Store in n_topics_assigned_0\n",
    "n_topics_assigned_0 = np.sum(theta_0 > .01)\n",
    "print(f'{n_topics_assigned_0 = :}\\n')\n",
    "\n",
    "# What are the indices of the assigned topics, sorted descending by the values in theta_0?\n",
    "#  Use np.argsort() to return the indices sorted by value (ascending)\n",
    "#  Use [::-1] to reverse the sorting order (from ascending to descending)\n",
    "#  Return only the first n_assigned_0 indices, those with large probability\n",
    "#  Store as assigned_topics_0\n",
    "#  You should see n_topics_assinged_0 indices\n",
    "assigned_topics_0 = np.argsort(theta_0[::-1][:n_topics_assigned_0])\n",
    "print(f'{assigned_topics_0 = :}')\n",
    "\n",
    "# Now that we have the topic indexes, we need to see what each topic looks like\n",
    "#   using the per topic word distrutions stored in lda.components_ (next question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic # 0 : rug resistant yes pad backing\n",
      "Topic # 1 : dial case strap bracelet watch\n",
      "Topic # 2 : clean_only wool only_imported only dry_clean\n",
      "Topic # 3 : upper sole rubber rubber_sole synthetic\n",
      "Topic # 4 : moisture wicking moisture_wicking fabric keep\n",
      "Topic # 5 : chair placemats foundation panties leaf\n",
      "Topic # 6 : safe dishwasher dishwasher_safe stainless_steel stainless\n",
      "Topic # 7 : silk wedge_sandals tie topper 3¼\n",
      "Topic # 8 : sleeveless line wash_line line_dry dry_imported\n",
      "Topic # 9 : sold what what_it ct color\n",
      "Topic #10 : socks fits_shoe shoe_sizes shoe nylon\n",
      "Topic #11 : shaft measurements by_size vary bamboo\n",
      "Topic #12 : short short_sleeves tee crewneck cotton_washable\n",
      "Topic #13 : silver jewelry_photos stones tone sterling\n",
      "Topic #14 : waist zip fly leg straight\n",
      "Topic #15 : comforter shams king twin queen\n",
      "Topic #16 : inseam pants rayon short rise\n",
      "Topic #17 : seat upholstery leather polyurethane frame\n",
      "Topic #18 : includes set bag interior top\n",
      "Topic #19 : wipe measures wipe_clean storage required\n"
     ]
    }
   ],
   "source": [
    "# 9. (5pts) Print Top Topic Terms\n",
    "\n",
    "# To get a sense of what each topic is composed of, we can print the most likely terms for each topic.\n",
    "# We'd like a print statement that looks like this:\n",
    "#     Topic # 0 : socks spandex fits shoe fits_shoe\n",
    "\n",
    "# To make indexing easier, first convert vocab from a list to np.array()\n",
    "# Store back into vocab\n",
    "vocab = np.array(vocab)\n",
    "\n",
    "# assert that vocab is the correct datatype\n",
    "assert type(vocab) is np.ndarray, \"vocab needs to be converted to a numpy array\"\n",
    "\n",
    "# For each topic print f'Topic #{topic_idx:2d} : ' followed by the top 5 most likely terms in that topic.\n",
    "# Hints: \n",
    "#   The per topic term distributions are stored in lda.components_ \n",
    "#      which should be a numpy array with shape (20, 5678)\n",
    "#   Iterate through the rows of lda.components_, one row per topic\n",
    "#   Use np.argsort() to get the sorted indices of the current row of lda.components_\n",
    "#      sorted by the values in that row in ascending order\n",
    "#   Use [::-1] to reverse the order of the sorted indices\n",
    "#   Use numpy array indexing to get the first 5 index values\n",
    "#   Use these indices to get the corresponding terms from vocab\n",
    "#   Join the list of terms with spaces using ' '.join()\n",
    "#   Each print statement should start with f'Topic #{topic_idx:2d} : ' \n",
    "#      where topic_idx is an integer 0 to 19\n",
    "# Each line should look similar to the example shown above\n",
    "\n",
    "# Use as many lines of code as you need\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    top_term_indices = np.argsort(topic)[::-1][:5]\n",
    "    top_terms = vocab[top_term_indices]\n",
    "    print(f'Topic #{topic_idx:2d} : {\" \".join(top_terms)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the description column of row 0, the assigned_topics_0 and \n",
    "# the top terms per topic above, our LDA model seems to have generated\n",
    "# topics that make sense given descriptions of department store goods, \n",
    "# with some a better fit than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate recommendations using topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 5000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10. (3pts) Generate Similarity Matrix\n",
    "\n",
    "# We'll use Content-Based Filtering to make recommendations based on a query product.\n",
    "# Each product will be represented by its LDA topic weights learned above (X_lda).\n",
    "# We'd like to recommend similar products in LDA space.\n",
    "# We'll use cosine distance as our measure of similarity, where lower distance means\n",
    "#  more similar.\n",
    "# Note that we're using \"distance\" where lower is better instead of \"similarity\" where higher is better\n",
    "#  as the default sorting is ascending and it makes indexing easier.\n",
    "\n",
    "# Import cosine_distances (not cosine_similarity) from sklearn.metrics.pairwise\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "# Use cosine_distances to generate similarity scores on our X_lda data\n",
    "# Store as distances\n",
    "# NOTE: we only need to pass X_lda in once as an argument,\n",
    "#   the function will calculate pairwise distance between all rows in that matrix\n",
    "distances = cosine_distances(X_lda)\n",
    "\n",
    "# print the shape of the distances matrix (should be 5000 x 5000)\n",
    "distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012'\n",
      " 'Timex® Easy Reader Womens White Leather Strap Watch T2H3917R'\n",
      " 'Bulova® Classic Mens Gold-Tone Stainless Steel Watch 97B146'\n",
      " 'Simplify Unisex The 2600 Navy Dial Leather-Band Watch SIM2607'\n",
      " 'Armitron® All Sport® Mens Black Dial Brown Leather Strap Chronograph Watch'\n",
      " 'Mossy Oak® Mens Orange Bezel Camouflage Silicone Strap Sport Watch'\n",
      " 'JBW The G4 Mens Diamond-Accent Light Brown Leather Strap Watch J6248LM'\n",
      " 'Claiborne® Mens Black and Silver-Tone Coin Edge Leather Watch'\n",
      " 'Whimsical Watches Personalized Chef Womens Silver-Tone Bezel Black Leather Strap Watch'\n",
      " 'Armitron® Mens Green Camo Digital Strap Watch']\n"
     ]
    }
   ],
   "source": [
    "# 11. (4pts) Find Recommended Products\n",
    "\n",
    "# Let's test our proposed recommendation engine using the product at row 0 in df_jcp.\n",
    "#   The name of this product is \"Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012\"\n",
    "#   Our system will recommend products similiar to this product.\n",
    "\n",
    "# Print the names for the top 10 most similar products to this query.\n",
    "# Suggested way to do this is:\n",
    "#   get the cosine distances from row 0 of the distances matrix\n",
    "#   get the indices of this first row of distances sorted by value ascending using np.argsort()\n",
    "#   get the first 10 indexes from this sorted array of indices\n",
    "#   use those indices to index into df_jcp.name_title \n",
    "#   to get the full string, use .values\n",
    "#   print the resulting array\n",
    "\n",
    "# HINT: The first two products will likely be:\n",
    "#   'Invicta® Sl Rally Mens Black Leather Strap Chronograph Watch 16012',\n",
    "#   'Timex® Easy Reader Womens White Leather Strap Watch T2H3917R',\n",
    "\n",
    "sorted_indices = np.argsort(distances[0])\n",
    "top_10_indices = sorted_indices[:10]\n",
    "recommended_products = df_jcp.name_title.iloc[top_10_indices].values\n",
    "print(recommended_products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment Analysis Using Pipelines\n",
    "\n",
    "Here we will train a model to classify positive vs negative sentiment on a set of pet supply product reviews using sklearn Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method DataFrame.info of                                                  review  rating\n",
      "0     My cats are considerably more happy with this ...       5\n",
      "1                                Made in China. No Good       5\n",
      "2     This is a great bowl. Nicely made and easy to ...       5\n",
      "3     Great hide for a Leopard Gecko as stated by ev...       4\n",
      "4     These cat claws went on easy, no problem what ...       5\n",
      "...                                                 ...     ...\n",
      "9995  This is the only ball that my English Bulldog ...       5\n",
      "9996     great deal for the price my dogs looooove them       5\n",
      "9997  Love it!! I don't have to check the water dish...       5\n",
      "9998  She likes it, the singing didn't last very lon...       4\n",
      "9999  The Gentle Leader is a lifesaver.  I've used i...       5\n",
      "\n",
      "[10000 rows x 2 columns]>\n",
      "\n",
      "Review in the first row: My cats are considerably more happy with this toy...and I don't have to leave the sofa to use it, given the long wand length. yay laziness!!\n",
      "Rating in the first row: 5\n"
     ]
    }
   ],
   "source": [
    "# 12. (2pts) Load the Data\n",
    "\n",
    "# The dataset we'll be working with is a set of product reviews\n",
    "#   of pet supply items on Amazon.\n",
    "# This data is taken from https://nijianmo.github.io/amazon/index.html\n",
    "#   \"Justifying recommendations using distantly-labeled reviews and fined-grained aspects\"\n",
    "#   Jianmo Ni, Jiacheng Li, Julian McAuley\n",
    "#   Empirical Methods in Natural Language Processing (EMNLP), 2019\n",
    "\n",
    "# Load product reviews from ../data/amazon-petsupply-reviews_subset.csv.zips\n",
    "# Use pandas read_csv function with the default parameters as in part 1.\n",
    "# Store the resulting dataframe as df_amzn.\n",
    "df_amzn = pd.read_csv('../data/amazon-petsupply-reviews_subset.csv.zip', \n",
    "                      compression = 'infer')\n",
    "\n",
    "# print a summary of df_amzn using .info()\n",
    "# there should be 10000 rows with 2 columns\n",
    "print(df_amzn.info)\n",
    "\n",
    "# print blank line\n",
    "print() \n",
    "\n",
    "# print the review in the first row of the dataframe as an example\n",
    "print(\"Review in the first row:\", df_amzn.loc[0, 'review'])\n",
    "\n",
    "# print the rating in the first row of the dataframe as an example\n",
    "print(\"Rating in the first row:\", df_amzn.loc[0, 'rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "5    0.66\n",
      "4    0.14\n",
      "3    0.09\n",
      "1    0.06\n",
      "2    0.05\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "rating\n",
      "True     0.66\n",
      "False    0.34\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 13. (2pts) Transform Target\n",
    "\n",
    "# The ratings are originally in a 5 point scale\n",
    "# We'll turn this into a binary classification task to approximate positive vs negative sentiment\n",
    "\n",
    "# Print the proportions of values seen in the rating column\n",
    "#  using value_counts() with normalize=True\n",
    "#  round to a precision of 2\n",
    "print(df_amzn['rating'].value_counts(normalize = True).round(2))\n",
    "\n",
    "# Create a new binary target by setting\n",
    "#  rows where rating is 5 to True\n",
    "#  rows where rating is not 5 to False\n",
    "# Store in y\n",
    "y = df_amzn['rating'] == 5\n",
    "\n",
    "# print a blank line\n",
    "print()\n",
    "\n",
    "# Print the proportions of values seen in y\n",
    "#  using value_counts() with normalize=True\n",
    "#  round to a precision of 2\n",
    "# True here means a rating of 5 (eg positive)\n",
    "# False means a rating less than 5 (eg negative)\n",
    "print(y.value_counts(normalize = True).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rating\n",
      "True     0.66\n",
      "False    0.34\n",
      "Name: proportion, dtype: float64\n",
      "rating\n",
      "True     0.66\n",
      "False    0.34\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 14. (2pts) Train-test split\n",
    "\n",
    "# Import train_test_split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split df_amzn.review and y into a train and test set\n",
    "#   using train_test_split\n",
    "#   stratifying by y\n",
    "#   with test_size = .2 \n",
    "#   and random_state = 512\n",
    "# Store as reviews_train,reviews_test,y_train,y_test\n",
    "reviews_train, reviews_test, y_train, y_test = train_test_split(df_amzn['review'], \n",
    "                                                                y, \n",
    "                                                                test_size = .2, \n",
    "                                                                stratify = y, \n",
    "                                                                random_state = 512)\n",
    "\n",
    "# print the proportion of values seen in y_train, round to a precision of 2\n",
    "print(y_train.value_counts(normalize = True).round(2))\n",
    "\n",
    "# visually compare this to the proportion of values seen in y\n",
    "#  to confirm that the class distributions are the same\n",
    "print(y.value_counts(normalize = True).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.5, min_df=5)),\n",
      "                ('gbc', GradientBoostingClassifier(n_estimators=20))])\n"
     ]
    }
   ],
   "source": [
    "# 15. (4pts) Create a Pipeline of TfIdf transformation and Classification\n",
    "\n",
    "# import Pipeline and GradientBoostingClassifier from sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Create a pipeline with two steps: \n",
    "#  TfIdfVectorizer with min_df=5 and max_df=.5 named 'tfidf'\n",
    "#  GradientBoostingClassifier with 20 trees named 'gbc'\n",
    "# Store as pipe_gbc\n",
    "pipe_gbc = Pipeline([('tfidf', TfidfVectorizer(min_df = 5, \n",
    "                                               max_df = .5)), \n",
    "                     ('gbc', GradientBoostingClassifier(n_estimators = 20))])\n",
    "\n",
    "# Print out the pipeline\n",
    "# You should see both steps: tfidf and gbc\n",
    "print(pipe_gbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'gbc__max_depth': 10, 'tfidf__ngram_range': (1, 1)}\n",
      "Best CV Score: 0.74\n"
     ]
    }
   ],
   "source": [
    "# 16. (5pts) Perform Grid Search on pipe_gbc\n",
    "\n",
    "# import GridSearchCV from sklearn\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Create a parameter grid to test using:\n",
    "#   unigrams or unigrams + bigrams in the tfidf step\n",
    "#   max_depth of 2 or 10 in the gbc step\n",
    "# Store as param_grid\n",
    "param_grid = {'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "              'gbc__max_depth': [2, 10]}\n",
    "\n",
    "# Instantiate GridSearchCV to evaluate pipe_gbc on the values in param_grid\n",
    "#   use cv=2 and n_jobs=-1 to reduce run time\n",
    "# Fit on the training set of reviews_train,y_train\n",
    "# Store as gs_pipe_gbc\n",
    "gs_pipe_gbc = GridSearchCV(pipe_gbc, \n",
    "                           param_grid, \n",
    "                           cv = 2, \n",
    "                           n_jobs = -1)\n",
    "gs_pipe_gbc.fit(reviews_train, y_train)\n",
    "\n",
    "# Print the best parameter settings in gs_pipe_gbc found by grid search\n",
    "print(\"Best Parameters:\", gs_pipe_gbc.best_params_)\n",
    "\n",
    "# Print the best cv score found by grid search, with a precision of 2\n",
    "print(\"Best CV Score:\", round(gs_pipe_gbc.best_score_, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# 17. (1 pts) Evaluate on the test set\n",
    "\n",
    "# Calculate the test set (reviews_test,y_test) score using the trained gs_pipe_gbc \n",
    "#   to give confidence that we have not overfit\n",
    "#   while still improving over a random baseline classifier\n",
    "# Print the accuracy score on the test set with a precision of 2\n",
    "test_score = gs_pipe_gbc.score(reviews_test, y_test)\n",
    "print(\"Test Set Accuracy:\", round(test_score, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for example reviews: [ True False]\n"
     ]
    }
   ],
   "source": [
    "# 18. (1 pts) Evaluate on example reviews\n",
    "\n",
    "# Generate predictions for these two sentences using the fit gs_pipe_gbc:\n",
    "#   'This is a great product.'\n",
    "#   'This product is not great.'\n",
    "# You should see True for the first (rating of 5) \n",
    "#   and False for the second (rating of less than 5)\n",
    "example_reviews = ['This is a great product.', 'This product is not great.']\n",
    "predictions = gs_pipe_gbc.predict(example_reviews)\n",
    "print(\"Predictions for example reviews:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
